Importing the Dependencies


[6]
12s
from google.colab import drive
drive.mount('/content/drive')
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).

[51]
1s
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

[52]
0s
import pandas as pd
credit_card_data = pd.read_csv('/content/Credit_Card_Fraud_Detection.csv')

[53]
0s
credit_card_data.head()

Next steps:

[9]
0s
credit_card_data.tail()


[11]
0s
credit_card_data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 690 entries, 0 to 689
Data columns (total 17 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   Unnamed: 0   690 non-null    int64  
 1   Customer_ID  690 non-null    int64  
 2   A_1          690 non-null    int64  
 3   A_2          690 non-null    float64
 4   A_3          690 non-null    float64
 5   A_4          690 non-null    int64  
 6   A_5          690 non-null    int64  
 7   A_6          690 non-null    int64  
 8   A_7          690 non-null    float64
 9   A_8          690 non-null    int64  
 10  A_9          690 non-null    int64  
 11  A_10         690 non-null    int64  
 12  A_11         690 non-null    int64  
 13  A_12         690 non-null    int64  
 14  A_13         690 non-null    int64  
 15  A_14         690 non-null    int64  
 16  class        690 non-null    int64  
dtypes: float64(3), int64(14)
memory usage: 91.8 KB

[12]
0s
credit_card_data.isnull().sum()


[15]
0s
credit_card_data['class'].value_counts()

This Dataset is highly unblanced

0 --> Normal Transaction

1 --> fraudulent transaction


[17]
0s
legit = credit_card_data[credit_card_data['class'] == 0]
fraud = credit_card_data[credit_card_data['class'] == 1]

[ ]
print(legit.shape)
print(fraud.shape)
(284315, 31)
(492, 31)

[24]
0s
legit['A_14'].describe()


[23]
0s
fraud['A_14'].describe()


[27]
0s
credit_card_data.groupby('class').mean()

Under-Sampling

Build a sample dataset containing similar distribution of normal transactions and Fraudulent Transactions

Number of Fraudulent Transactions --> 492


[33]
0s
legit_sample = legit.sample(n=383)
Concatenating two DataFrames


[34]
0s
new_dataset = pd.concat([legit_sample, fraud], axis=0)

[35]
0s
new_dataset.head()

Next steps:

[36]
0s
new_dataset.tail()


[54]
0s
new_dataset['class'].value_counts()


[42]
0s
new_dataset.groupby('class').mean()

Splitting the data into Features & Targets


[44]
0s
X = new_dataset.drop(columns='class', axis=1)
Y = new_dataset['class']

[45]
0s
print(X)
     Unnamed: 0  Customer_ID  A_1    A_2    A_3  A_4  A_5  A_6    A_7  A_8  \
680         680     15790689    1  21.17   0.00    2    8    4  0.500    0   
262         262     15734578    0  21.75  11.75    2    8    4  0.250    0   
397         397     15603565    0  22.92   1.25    2   11    4  0.250    0   
160         160     15798895    1  34.08   2.50    2    8    4  1.000    0   
334         334     15795527    0  42.25   1.75    1    8    4  0.000    0   
..          ...          ...  ...    ...    ...  ...  ...  ...    ...  ...   
684         684     15740356    1  43.00   0.29    1   13    8  1.750    1   
685         685     15808223    1  31.57  10.50    2   14    4  6.500    1   
687         687     15675450    0  18.83   9.54    2    6    4  0.085    1   
688         688     15776494    0  27.42  14.50    2   14    8  3.085    1   
689         689     15592412    1  41.00   0.04    2   10    4  0.040    0   

     A_9  A_10  A_11  A_12  A_13  A_14  
680    0     0     1     1     0     1  
262    0     0     1     2   180     1  
397    0     0     1     2   120   810  
160    0     0     0     2   460    17  
334    0     0     1     2   150     2  
..   ...   ...   ...   ...   ...   ...  
684    1     8     0     2   100   376  
685    0     0     0     2     0     1  
687    0     0     0     2   100     1  
688    1     1     0     2   120    12  
689    1     1     0     1   560     1  

[690 rows x 16 columns]

[46]
0s
print(Y)
680    0
262    0
397    0
160    0
334    0
      ..
684    1
685    1
687    1
688    1
689    1
Name: class, Length: 690, dtype: int64
Split the data into Training data & Testing Data

Model Training

Logistic Regression


[56]
0s
model = LogisticRegression()

[ ]
# training the Logistic Regression Model with Training Data
model.fit(X_train, Y_train)

Model Evaluation

Accuracy Score


[68]
0s
# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

[71]
0s
print('Accuracy score on Test Data : ', test_data_accuracy)
Accuracy score on Test Data :  0.8623188405797102

[70]
0s
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

[ ]

Start coding or generate with AI.

[72]
0s
from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, confusion_matrix,
                            classification_report)
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42, stratify=Y
)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_train, Y_train)
train_pred = model.predict(X_train)
train_probs = model.predict_proba(X_train)[:, 1]
print("=== Training Set Performance ===")
print(f"Accuracy: {accuracy_score(Y_train, train_pred):.4f}")
print(f"Precision: {precision_score(Y_train, train_pred):.4f}")
print(f"Recall: {recall_score(Y_train, train_pred):.4f}")
print(f"F1-Score: {f1_score(Y_train, train_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(Y_train, train_probs):.4f}")
print("\nClassification Report:")
print(classification_report(Y_train, train_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(Y_train, train_pred))
test_pred = model.predict(X_test)
test_probs = model.predict_proba(X_test)[:, 1]
print("\n=== Test Set Performance ===")
print(f"Accuracy: {accuracy_score(Y_test, test_pred):.4f}")
print(f"Precision: {precision_score(Y_test, test_pred):.4f}")
print(f"Recall: {recall_score(Y_test, test_pred):.4f}")
print(f"F1-Score: {f1_score(Y_test, test_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(Y_test, test_probs):.4f}")
print("\nClassification Report:")
print(classification_report(Y_test, test_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(Y_test, test_pred))
=== Training Set Performance ===
Accuracy: 1.0000
Precision: 1.0000
Recall: 1.0000
F1-Score: 1.0000
ROC AUC: 1.0000

Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       306
           1       1.00      1.00      1.00       246

    accuracy                           1.00       552
   macro avg       1.00      1.00      1.00       552
weighted avg       1.00      1.00      1.00       552


Confusion Matrix:
[[306   0]
 [  0 246]]

=== Test Set Performance ===
Accuracy: 0.8623
Precision: 0.8281
Recall: 0.8689
F1-Score: 0.8480
ROC AUC: 0.9542

Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.86      0.87        77
           1       0.83      0.87      0.85        61

    accuracy                           0.86       138
   macro avg       0.86      0.86      0.86       138
weighted avg       0.86      0.86      0.86       138


Confusion Matrix:
[[66 11]
 [ 8 53]]
task 5.jpg

Colab paid products - Cancel contracts here
